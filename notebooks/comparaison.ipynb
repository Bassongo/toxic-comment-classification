{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des Modeles de Classification de Toxicite\n",
    "\n",
    "Ce notebook compare les performances de 3 modeles:\n",
    "1. **Modele Hybride** (Naive Bayes / Logistic Regression / Random Forest)\n",
    "2. **XGBoost**\n",
    "3. **BERT** (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "LABEL_COLS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement des Donnees de Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "print(f\"Dataset: {len(df)} commentaires\")\n",
    "\n",
    "X = df['comment_text'].values\n",
    "y = df[LABEL_COLS].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f\"Train: {len(X_train)}\")\n",
    "print(f\"Validation: {len(X_val)}\")\n",
    "print(f\"Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Modeles\n",
    "### 2.1 Modele Hybride (NB/LR/RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid_model import HybridToxicityClassifier\n",
    "\n",
    "try:\n",
    "    hybrid_model = HybridToxicityClassifier.load('toxicity_classifier.pkl')\n",
    "    hybrid_model.print_config()\n",
    "    HYBRID_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Erreur chargement modele hybride: {e}\")\n",
    "    HYBRID_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Modele XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_model import ToxicCommentClassifier\n",
    "\n",
    "try:\n",
    "    xgb_model = ToxicCommentClassifier.load('toxic_classifier.pkl')\n",
    "    print(\"Modele XGBoost charge avec succes\")\n",
    "    print(f\"Seuils optimaux: {xgb_model.thresholds}\")\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print(f\"Erreur chargement modele XGBoost: {e}\")\n",
    "    XGBOOST_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Modele BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\n\nBERT_AVAILABLE = False\n\ntry:\n    from transformers import BertModel, BertTokenizer\n    \n    # Architecture identique a celle utilisee sur SageMaker\n    class BertToxicClassifier(nn.Module):\n        def __init__(self, num_labels=6, dropout=0.3):\n            super().__init__()\n            self.bert = BertModel.from_pretrained('bert-base-uncased')\n            self.dropout = nn.Dropout(dropout)\n            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n        \n        def forward(self, input_ids, attention_mask):\n            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n            pooled_output = outputs.last_hidden_state[:, 0, :]\n            pooled_output = self.dropout(pooled_output)\n            logits = self.classifier(pooled_output)\n            return logits\n    \n    bert_tokenizer = BertTokenizer.from_pretrained('bert_results/bert_tokenizer')\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    bert_model = BertToxicClassifier(num_labels=6)\n    bert_model.load_state_dict(torch.load('bert_results/bert_toxic_best.pt', map_location=device))\n    bert_model.to(device)\n    bert_model.eval()\n    \n    print(f\"Modele BERT charge avec succes sur {device}\")\n    BERT_AVAILABLE = True\n    \nexcept Exception as e:\n    print(f\"Erreur chargement BERT: {e}\")\n    print(\"Utilisation des resultats SageMaker\")\n    BERT_AVAILABLE = False"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictions sur le Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "SAMPLE_SIZE = min(1000, len(X_test))\n",
    "X_sample = X_test[:SAMPLE_SIZE]\n",
    "y_sample = y_test[:SAMPLE_SIZE]\n",
    "print(f\"Evaluation sur {SAMPLE_SIZE} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Predictions Modele Hybride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYBRID_AVAILABLE:\n",
    "    print(\"Predictions avec le modele Hybride...\")\n",
    "    hybrid_preds = hybrid_model.predict_batch(X_sample.tolist())\n",
    "    y_pred_hybrid = hybrid_preds[LABEL_COLS].values\n",
    "    \n",
    "    results['Hybride'] = {\n",
    "        'predictions': y_pred_hybrid,\n",
    "        'f1_micro': f1_score(y_sample, y_pred_hybrid, average='micro'),\n",
    "        'f1_macro': f1_score(y_sample, y_pred_hybrid, average='macro'),\n",
    "        'hamming': hamming_loss(y_sample, y_pred_hybrid)\n",
    "    }\n",
    "    print(f\"F1 micro: {results['Hybride']['f1_micro']:.4f}\")\n",
    "    print(f\"F1 macro: {results['Hybride']['f1_macro']:.4f}\")\n",
    "else:\n",
    "    print(\"Modele Hybride non disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predictions Modele XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"Predictions avec le modele XGBoost...\")\n",
    "    xgb_preds = xgb_model.predict(X_sample.tolist())\n",
    "    y_pred_xgb = xgb_preds[LABEL_COLS].values\n",
    "    \n",
    "    results['XGBoost'] = {\n",
    "        'predictions': y_pred_xgb,\n",
    "        'f1_micro': f1_score(y_sample, y_pred_xgb, average='micro'),\n",
    "        'f1_macro': f1_score(y_sample, y_pred_xgb, average='macro'),\n",
    "        'hamming': hamming_loss(y_sample, y_pred_xgb)\n",
    "    }\n",
    "    print(f\"F1 micro: {results['XGBoost']['f1_micro']:.4f}\")\n",
    "    print(f\"F1 macro: {results['XGBoost']['f1_macro']:.4f}\")\n",
    "else:\n",
    "    print(\"Modele XGBoost non disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Predictions Modele BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BERT_AVAILABLE:\n",
    "    print(\"Predictions avec le modele BERT...\")\n",
    "    \n",
    "    def predict_bert(texts, model, tokenizer, device, batch_size=32):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            encoding = tokenizer(list(batch_texts), padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                all_preds.extend(preds)\n",
    "        return np.array(all_preds)\n",
    "    \n",
    "    y_pred_bert = predict_bert(X_sample, bert_model, bert_tokenizer, device)\n",
    "    results['BERT'] = {\n",
    "        'predictions': y_pred_bert,\n",
    "        'f1_micro': f1_score(y_sample, y_pred_bert, average='micro'),\n",
    "        'f1_macro': f1_score(y_sample, y_pred_bert, average='macro'),\n",
    "        'hamming': hamming_loss(y_sample, y_pred_bert)\n",
    "    }\n",
    "    print(f\"F1 micro: {results['BERT']['f1_micro']:.4f}\")\n",
    "    print(f\"F1 macro: {results['BERT']['f1_macro']:.4f}\")\n",
    "else:\n",
    "    print(\"BERT non disponible localement - Resultats SageMaker:\")\n",
    "    results['BERT'] = {\n",
    "        'f1_micro': 0.7947,\n",
    "        'f1_macro': 0.6774,\n",
    "        'hamming': 0.0149,\n",
    "        'f1_per_label': {'toxic': 0.8348, 'severe_toxic': 0.4469, 'obscene': 0.8284, 'threat': 0.6263, 'insult': 0.7824, 'identity_hate': 0.5455}\n",
    "    }\n",
    "    print(f\"F1 micro: {results['BERT']['f1_micro']:.4f}\")\n",
    "    print(f\"F1 macro: {results['BERT']['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaison des Resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON DES MODELES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Modele':<20} {'F1 Micro':<12} {'F1 Macro':<12} {'Hamming Loss'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<20} {metrics['f1_micro']:<12.4f} {metrics['f1_macro']:<12.4f} {metrics['hamming']:.4f}\")\n",
    "\n",
    "best_model = max(results.items(), key=lambda x: x[1]['f1_micro'])\n",
    "print(f\"\\nMeilleur modele (F1 micro): {best_model[0]} ({best_model[1]['f1_micro']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "models = list(results.keys())\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# F1 Micro\n",
    "ax1 = axes[0]\n",
    "f1_micro_values = [results[m]['f1_micro'] for m in models]\n",
    "bars1 = ax1.bar(models, f1_micro_values, color=colors[:len(models)])\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('F1 Score Micro')\n",
    "ax1.set_ylim(0, 1)\n",
    "for bar, val in zip(bars1, f1_micro_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{val:.3f}', ha='center')\n",
    "\n",
    "# F1 Macro\n",
    "ax2 = axes[1]\n",
    "f1_macro_values = [results[m]['f1_macro'] for m in models]\n",
    "bars2 = ax2.bar(models, f1_macro_values, color=colors[:len(models)])\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('F1 Score Macro')\n",
    "ax2.set_ylim(0, 1)\n",
    "for bar, val in zip(bars2, f1_macro_values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{val:.3f}', ha='center')\n",
    "\n",
    "# Hamming Loss\n",
    "ax3 = axes[2]\n",
    "hamming_values = [results[m]['hamming'] for m in models]\n",
    "bars3 = ax3.bar(models, hamming_values, color=colors[:len(models)])\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Hamming Loss (plus bas = mieux)')\n",
    "for bar, val in zip(bars3, hamming_values):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002, f'{val:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparaison_modeles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. F1 Score par Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_per_label = {}\n",
    "\n",
    "for model_name, data in results.items():\n",
    "    if 'predictions' in data:\n",
    "        f1_per_label[model_name] = {}\n",
    "        for i, label in enumerate(LABEL_COLS):\n",
    "            f1 = f1_score(y_sample[:, i], data['predictions'][:, i], zero_division=0)\n",
    "            f1_per_label[model_name][label] = f1\n",
    "    elif 'f1_per_label' in data:\n",
    "        f1_per_label[model_name] = data['f1_per_label']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"F1 SCORE PAR LABEL\")\n",
    "print(\"=\" * 80)\n",
    "header = f\"{'Label':<15}\" + \"\".join([f\"{m:<15}\" for m in f1_per_label.keys()])\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for label in LABEL_COLS:\n",
    "    row = f\"{label:<15}\"\n",
    "    for model_name in f1_per_label.keys():\n",
    "        row += f\"{f1_per_label[model_name].get(label, 0):<15.4f}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f1_per_label:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(LABEL_COLS))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, (model_name, scores) in enumerate(f1_per_label.items()):\n",
    "        values = [scores.get(label, 0) for label in LABEL_COLS]\n",
    "        offset = (i - len(f1_per_label)/2 + 0.5) * width\n",
    "        ax.bar(x + offset, values, width, label=model_name, color=colors[i])\n",
    "    \n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('F1 Score par Label et par Modele')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(LABEL_COLS, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('f1_par_label_comparaison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "======================================================================\n",
    "CONCLUSIONS\n",
    "======================================================================\n",
    "\n",
    "RESUME DES MODELES:\n",
    "\n",
    "1. MODELE HYBRIDE (NB/LR/RF)\n",
    "   - Rapide et interpretable\n",
    "   - Peu de ressources necessaires\n",
    "   - Performance limitee sur classes rares\n",
    "\n",
    "2. XGBOOST\n",
    "   - Bon equilibre performance/vitesse\n",
    "   - Necessite TF-IDF (feature engineering)\n",
    "   - Seuils optimisables par label\n",
    "\n",
    "3. BERT (Deep Learning)\n",
    "   - Meilleure performance globale\n",
    "   - Comprend le contexte semantique\n",
    "   - Necessite GPU et plus de ressources\n",
    "\n",
    "RECOMMANDATIONS:\n",
    "- Production avec contraintes de latence: Hybride ou XGBoost\n",
    "- Meilleure precision: BERT\n",
    "- Bon compromis: XGBoost avec seuils optimises\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test sur des Exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = [\n",
    "    \"You are an idiot and I hate you!\",\n",
    "    \"This is a great article, thanks for sharing!\",\n",
    "    \"I will find you and hurt you\",\n",
    "    \"What a stupid and ugly person\",\n",
    "    \"Thanks for the helpful information\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST SUR DES EXEMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for comment in test_comments:\n",
    "    print(f\"\\nCommentaire: \\\"{comment}\\\"\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if HYBRID_AVAILABLE:\n",
    "        pred = hybrid_model.predict(comment)\n",
    "        labels = [l for l, v in pred.items() if v == 1]\n",
    "        print(f\"  Hybride: {labels if labels else 'Non toxique'}\")\n",
    "    \n",
    "    if XGBOOST_AVAILABLE:\n",
    "        pred = xgb_model.predict(comment)\n",
    "        labels = [l for l in LABEL_COLS if pred.iloc[0][l] == 1]\n",
    "        print(f\"  XGBoost: {labels if labels else 'Non toxique'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}